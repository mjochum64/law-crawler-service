# Legal Document Crawler Service

Ein Spring Boot Service zum Crawlen von Rechtsprechung, Gesetzen und Verwaltungsvorschriften von https://www.rechtsprechung-im-internet.de

## ğŸš€ Features

- **Automatisches Crawling** von XML-Dokumenten Ã¼ber Sitemaps
- **Strukturierte Speicherung** nach Gericht, Jahr und Monat
- **Rate Limiting** und ethisches Crawling
- **Scheduling** fÃ¼r automatisierte Crawling-Sessions
- **REST API** fÃ¼r Steuerung und Monitoring
- **H2 Database** fÃ¼r Metadaten-Speicherung
- **Spring Boot Actuator** fÃ¼r Health Checks

## ğŸ›ï¸ UnterstÃ¼tzte Gerichte

- **BAG** - Bundesarbeitsgericht
- **BGH** - Bundesgerichtshof  
- **BSG** - Bundessozialgericht
- **BVerwG** - Bundesverwaltungsgericht

## ğŸ“‹ Voraussetzungen

- Java 17+
- Maven 3.8+

## ğŸ› ï¸ Installation & Start

```bash
# Repository klonen
git clone <repository-url>
cd law-crawler-service

# AbhÃ¤ngigkeiten installieren und starten
mvn spring-boot:run

# Alternative: JAR erstellen und ausfÃ¼hren
mvn clean package
java -jar target/law-crawler-service-1.0.0-SNAPSHOT.jar
```

## ğŸ”§ Konfiguration

Konfiguration Ã¼ber `application.yml`:

```yaml
crawler:
  base-url: https://www.rechtsprechung-im-internet.de
  rate-limit-ms: 2000  # 2 Sekunden zwischen Requests
  storage:
    base-path: ./legal-documents
  scheduled:
    enabled: true
    days-back: 7
```

## ğŸŒ REST API

### Crawling starten
```bash
POST /api/crawler/crawl?date=2024-08-06&forceUpdate=false
```

### Status abfragen
```bash
GET /api/crawler/status
```

### Dokumente suchen
```bash
GET /api/crawler/search?query=arbeitsrecht
```

### Fehlgeschlagene Downloads wiederholen
```bash
POST /api/crawler/retry-failed
```

### Dokumente nach Gericht
```bash
GET /api/crawler/documents/BAG?page=0&size=20
```

## ğŸ“ Verzeichnisstruktur

Dokumente werden strukturiert gespeichert:

```
legal-documents/
â”œâ”€â”€ bag/          # Bundesarbeitsgericht
â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”œâ”€â”€ 01/   # Januar
â”‚   â”‚   â””â”€â”€ 02/   # Februar
â”œâ”€â”€ bgh/          # Bundesgerichtshof
â”œâ”€â”€ bsg/          # Bundessozialgericht
â””â”€â”€ bverwg/       # Bundesverwaltungsgericht
```

## â° Automatisierung

### Scheduling-Optionen

- **TÃ¤glich 6:00 Uhr**: Crawling der letzten 7 Tage
- **Sonntags 2:00 Uhr**: VollstÃ¤ndiger Crawl der letzten 30 Tage
- **Alle 6 Stunden**: Wiederholung fehlgeschlagener Downloads
- **StÃ¼ndlich**: System Health Check

## ğŸ“Š Monitoring

### H2 Console
- URL: http://localhost:8080/h2-console
- JDBC URL: `jdbc:h2:file:./data/legal-documents`
- Username: `sa`

### Actuator Endpoints
- Health: http://localhost:8080/actuator/health
- Metrics: http://localhost:8080/actuator/metrics
- Info: http://localhost:8080/actuator/info

## ğŸ§ª Tests

```bash
# Unit Tests ausfÃ¼hren
mvn test

# Integration Tests
mvn verify

# Coverage Report
mvn jacoco:report
```

## âš–ï¸ Rechtliche Hinweise

- **Respektiert robots.txt** der Zielwebseite
- **Rate Limiting** implementiert (2 Sekunden zwischen Requests)
- **Nur Ã¶ffentlich verfÃ¼gbare** Dokumente
- **Keine rechtliche Beratung** durch diesen Service

## ğŸ”§ Entwicklung

### Architektur

- **CrawlerOrchestrationService**: Zentrale Koordination
- **SitemapCrawlerService**: Sitemap-Parsing
- **DocumentDownloadService**: XML-Download und -Speicherung  
- **ScheduledCrawlerService**: Automatisierte AusfÃ¼hrung
- **LegalDocument**: JPA Entity fÃ¼r Metadaten

### Technologie-Stack

- Spring Boot 3.2.2
- Java 17
- H2 Database
- JSoup fÃ¼r HTML/XML-Parsing
- Apache HttpComponents
- Spring Data JPA
- Spring Scheduling

## ğŸ“ Logs

Logs werden gespeichert in `./logs/crawler.log`

- Rotation: 10MB pro Datei
- Historie: 30 Dateien
- Level: INFO (konfigurierbar)

## ğŸš¨ Troubleshooting

### HÃ¤ufige Probleme

1. **403 Forbidden**: robots.txt verbietet Crawling - kontaktiere kompetenzzentrum-ris@bfj.bund.de
2. **OutOfMemory**: JVM Heap Size erhÃ¶hen (`-Xmx2g`)
3. **Speicherplatz**: RegelmÃ¤ÃŸige Bereinigung alter Dateien

### Logs prÃ¼fen
```bash
tail -f logs/crawler.log
```

## ğŸ¤ Beitragen

1. Fork des Repositories
2. Feature Branch erstellen
3. Tests hinzufÃ¼gen
4. Pull Request erstellen

## ğŸ“„ Lizenz

Dieses Projekt steht unter der [MIT Lizenz](LICENSE).