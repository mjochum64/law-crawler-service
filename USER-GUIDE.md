> Wie kann ich Dokumente nun fÃ¼r einen bestimmten zeitraum abrufen

â— ğŸ“… ZEITRAUM-BASIERTES CRAWLING - Anleitung

  Das System bietet mehrere Methoden fÃ¼r zeitraumbasiertes Abrufen von Dokumenten:

  ğŸš€ 1. Sofortiges Crawling fÃ¼r spezifische Daten

  Einzelnes Datum crawlen:

  # FÃ¼r heute (gestern wird standardmÃ¤ÃŸig gecrawlt)
  curl -X POST "http://localhost:8080/api/crawler/crawl"

  # FÃ¼r ein bestimmtes Datum
  curl -X POST "http://localhost:8080/api/crawler/crawl?date=2024-08-01"

  # Mit Force Update (bereits heruntergeladene Dokumente erneut verarbeiten)
  curl -X POST "http://localhost:8080/api/crawler/crawl?date=2024-08-01&forceUpdate=true"

  Status Ã¼berwachen:

  curl http://localhost:8080/api/crawler/status

  ğŸ“… 2. Zeitraum-Crawling (mehrere Tage)

  FÃ¼r grÃ¶ÃŸere ZeitrÃ¤ume kÃ¶nnen Sie ein einfaches Script verwenden:

  #!/bin/bash
  # Crawl-Script fÃ¼r Zeitraum (crawl-range.sh)

  START_DATE="2024-07-01"
  END_DATE="2024-08-06"

  current_date=$START_DATE

  while [ "$current_date" != $(date -I -d "$END_DATE + 1 day") ]; do
      echo "Crawling fÃ¼r Datum: $current_date"

      curl -X POST "http://localhost:8080/api/crawler/crawl?date=$current_date"

      # 30 Sekunden warten zwischen den Requests
      sleep 30

      # NÃ¤chster Tag
      current_date=$(date -I -d "$current_date + 1 day")
  done

  echo "Crawling fÃ¼r Zeitraum $START_DATE bis $END_DATE abgeschlossen"

  â° 3. Automatisches Scheduling (bereits konfiguriert)

  Das System ist bereits fÃ¼r automatisches Crawling konfiguriert:

  crawler:
    scheduled:
      enabled: true
      daily-cron: "0 0 6 * * ?"      # TÃ¤glich um 6:00 Uhr
      weekly-cron: "0 0 2 * * SUN"   # Sonntag um 2:00 Uhr  
      retry-cron: "0 0 */6 * * ?"     # Alle 6 Stunden Retry

  ğŸ“Š 4. Abgerufene Dokumente verwalten

  Status und Statistiken:

  # System-Status
  curl http://localhost:8080/api/crawler/status

  # Statistiken nach Gerichten
  curl http://localhost:8080/api/crawler/statistics/courts

  Dokumente nach Gericht abrufen:

  # BGH Dokumente (erste 20)
  curl "http://localhost:8080/api/crawler/documents/BGH"

  # Mit Pagination
  curl "http://localhost:8080/api/crawler/documents/BGH?page=0&size=50"

  # Andere Gerichte
  curl "http://localhost:8080/api/crawler/documents/BAG"  # Bundesarbeitsgericht
  curl "http://localhost:8080/api/crawler/documents/BSG"  # Bundessozialgericht
  curl "http://localhost:8080/api/crawler/documents/BVerwG"  # Bundesverwaltungsgericht

  Dokumente durchsuchen:

  # Textsuche in Titeln
  curl "http://localhost:8080/api/crawler/search?query=Arbeitsrecht"

  ğŸ’¾ 5. Gespeicherte Dokumente finden

  Die XML-Dateien werden strukturiert gespeichert:

  ./legal-documents/
  â”œâ”€â”€ bgh/2024/08/KORE600068723.xml
  â”œâ”€â”€ bag/2024/07/KARE500041892.xml
  â”œâ”€â”€ bsg/2024/06/KSRE780023456.xml
  â””â”€â”€ bverwg/2024/05/WBRE920012345.xml

  Dateisystem durchsuchen:

  # Alle XML-Dateien fÃ¼r August 2024
  find ./legal-documents -path "*/2024/08/*.xml"

  # BGH Dokumente fÃ¼r 2024
  find ./legal-documents/bgh/2024 -name "*.xml"

  # Anzahl Dokumente pro Gericht
  find ./legal-documents -name "*.xml" | cut -d/ -f3 | sort | uniq -c

  ğŸ”„ 6. Fehlerbehandlung und Retry

  # Fehlgeschlagene Dokumente erneut versuchen
  curl -X POST "http://localhost:8080/api/crawler/retry-failed"

  ğŸ“ˆ 7. Monitoring und Health Check

  # Application Health
  curl http://localhost:8080/actuator/health

  # Detailed Health Info
  curl http://localhost:8080/actuator/health | jq .

  # System Metrics
  curl http://localhost:8080/actuator/metrics

  âš™ï¸ 8. Erweiterte Konfiguration

  Sie kÃ¶nnen das Crawling-Verhalten in application.yml anpassen:

  crawler:
    rate-limit-ms: 2000        # Pause zwischen Requests (2 Sekunden)
    base-url: https://www.rechtsprechung-im-internet.de
    storage:
      base-path: ./legal-documents

    scheduled:
      enabled: true
      days-back: 7              # Wie weit zurÃ¼ck crawlen

  ğŸ¯ Empfohlener Workflow:

  1. Starten Sie mit einem kleinen Zeitraum (1-2 Tage)
  2. Ãœberwachen Sie den Status regelmÃ¤ÃŸig
  3. PrÃ¼fen Sie die Dateisystem-Ausgabe
  4. Skalieren Sie auf grÃ¶ÃŸere ZeitrÃ¤ume
  5. Aktivieren Sie Scheduling fÃ¼r kontinuierliches Crawling

  Das System respektiert automatisch die robots.txt BeschrÃ¤nkungen und implementiert Rate Limiting fÃ¼r ethisches Crawling.
